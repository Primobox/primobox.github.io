
var documents = [{
    "id": 0,
    "url": "http://0.0.0.0:4000/404.html",
    "title": "404",
    "body": "404 Cette page n'existe pas !Veuillez utiliser le champ de recherche ou retournez sur notre page d'accueil ! "
    }, {
    "id": 1,
    "url": "http://0.0.0.0:4000/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 2,
    "url": "http://0.0.0.0:4000/",
    "title": "Home",
    "body": "      Articles:                                                                                                                                         Post Mortem              :       Retour d'expérience de la refonte d'une application qui ne s'est pas passée exactement comme on aurait pu l'espérer:                                                                                                                                   Vincent Dubois, Damien Boué                9 août 2022                           5 minutes                                                                                                                                                                Les microservices, c'est pas automatique !              :       Depuis maintenant des années, on parle souvent des microservices comme de la solution à tous les problèmes du monde du logiciel. Alors non seulement l'idée ne date pas d'hier, mais surtout est-ce si bien que cela ?:                                                                                           Alexandre FIllatre                6 mars 2022                           19 minutes                                                                                                                                                                Ma carrière de jeune développeur              :       Cela fait maintenant 5 ans que j'ai terminé mes études et pourtant je n'ai eu de cesse de revoir ma vision de ce qu'est un développeur. Voici une rétrospective du premier changement de cette vision. :                                                                                           Damien Boué                23 février 2022                           7 minutes                                                                                                                                                                Le “Messager” : un projet pas comme les autres chez Primobox              :       Trouver une entreprise dans laquelle on peut progresser, apprendre, innover, tout en forgeant un logiciel de qualité et des compétences partageables à tout le monde, n’est pas chose courante. Primobox est de cette trempe là. Laissez-moi vous raconter brièvement cette aventure que j’ai eu la chance de vivre. :                                                                                           Guillaume Saint Etienne                2 février 2022                           20 minutes                                                                                                                                                                L'Outside-in Diamond TDD, ou l'art de mieux tester              :       Comment écrire dans une architecture hexagonale des tests orientés métier, rapides, et qui vont couvrir le plus de code possible. :                                                                                           Vincent Dubois                15 janvier 2022                           7 minutes                                    "
    }, {
    "id": 3,
    "url": "http://0.0.0.0:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 4,
    "url": "http://0.0.0.0:4000/post-mortem/",
    "title": "Post Mortem",
    "body": "2022/08/09 - La genèse: Le projet de refonte de l’application myPrimobox a été démarré en 2021. Début 2021, le produit existant comporte plusieurs problèmes majeurs qui nous invitent à revoir notre copie :  problèmes de performances en production qui donnent lieu à de multiples indisponibilités de la plateforme problèmes de conception qui nous amènent à agir directement sur les données des utilisateurs en production pour “patcher” ergonomie vieillissante qui met en difficulté nos utilisateurs finaux en fracture numériqueC’est à la fin du deuxième trimestre 2021 que le projet est officiellement démarré Une stratégie à plusieurs niveaux: Changement d’équipe: L’équipe de départ est conposée de deux développeurs plutôt juniors, notre Product Owner ainsi qu’un UX designer qui intervient lors de la phase de lancement. Très vite, des petits problèmes apparaissent : le manque de séniorité de l’équipe de développement interroge notre CTO qui doit guider l’équipe pour rectifier le tir. Les refactorings s’enchainent : il faut sécuriser l’application dans le temps en s’appuyant sur une architecture hexagonale, mais également en utilisant le Domain-driven design. L’équipe s’essoufle un peu et peine à monter en compétences correctement sur ces sujets. En parallèle, le projet ne s’arrête pas, loin de là. Des maquettes, ainsi que des tests utilisateurs sont réalisés, le projet doit avancer. L’équipe bouge, un développeur prestataire finit sa mission, il est remplacé par un développeur interne qui a plus d’expérience. Celui-ci monte en compétences très vite. Les semaines et les mois passent. Trois autres développeurs arrivent sur le projet, dont deux qui ont de bonnes compétences en Domain-driven design. Avec ce changement d’équipe, certains choix qui ont été fait précédemment posent des questions quant à la perennité du projet :  Pourquoi être parti sur de la programmation réactive alors que l’équipe n’avait aucune compétence dessus ? Pourquoi avoir développé de zéro des fonctionnalités autour de la connexion des utilisateurs alors qu’il existe des solutions sur l’étagère qui font ça très bien ? Mais surtout : quelle est la raison d’être de cette refonte ?Le big bang: Le vrai challenge technique sur ce projet de refonte ne vient pas forcément de la plateforme elle-même. Il tient surtout à la base d’utilisateurs existants qui est assez conséquente (plusieurs centaines de milliers). Que faut-il faire ? Doit-on migrer les données de tous les utilisateurs ? Certains ont des comptes “simples”, mais certains ont des comptes dits “complexes”, qui demandent plus de temps pour savoir comment les gérer. Que faire des comptes inactifs (personnes qui ne se sont finalement jamais connectées à la plateforme) ? C’est pour cela qu’est né le projet de migration des données. C’est une application batch à part de l’application de refonte qui tournera pour migrer les comptes de nos utilisateurs le jour de la mise en production (on peut l’appeler jour de la bascule). C’est donc une migration big bang qui est choisie : on migre tout d’un seul coup. Premier hic : on ne peut en aucun cas dire combien de temps la migration des données va prendre le jour de la mise en production. Que se passera-t-il si cela dure plusieurs heures ? ou plusieurs jours ? Cela pourrait amener une grande indisponibilité pour nos clients, et c’est hors de question. Les semaines avancent, et l’équipe n’arrive pas à donner un état d’avancement précis sur ce projet aux personnes du pôle produit, cela crée des tensions. L’équipe de développement commence à se rendre compte que cette migration big bang est très éloignée des principes de l’agilité que l’équipe suit scrupuleusement en dehors de ça. La remise en question: Début 2022, l’équipe travaille à la fois sur les fonctionnalités de la refonte et sur le batch de migration des données. Et le moins qu’on puisse dire, c’est que la situation, et le mode “big bang” ne convient pas à l’équipe. On sent que l’on va droit dans le mur. On décide donc de “poser les stylos”, et de réfléchir à l’avenir de notre projet. Que fait-on ? Doit-on continuer ? Doit-on abandonner ? Le pôle produit va-t-il nous suivre dans notre démarche ? Il faut remettre les pendules à l’heure et faire réaliser à toutes les parties prenantes que l’on ne peut pas continuer le projet tel qu’il est actuellement. Cette vision “tout ou rien” des choses ne peut pas fonctionner. En tout cas, on ne pouvait pas continuer à travailler sur cette migration sans avoir un minimum de vision sur les grandes étapes de la refonte, car on allait droit dans le mur. Ce n’est pas très agile de prévoir une migration des données “big bang” qui s’additionne à une refonte qui part en production du jour au lendemain. Qui plus est, en parallèle, les demandes de nos clients nous remontent par le biais de nos commerciaux, et on n’y répond pas puisque rien n’est livré en production. On a donc réfléchi à l’ensemble des solutions qui s’offraient à nous, et pour chacune nous avons essayé de faire un macro-planning en notant toutes les grandes actions à effectuer. Virage stratégique: Et si la solution venait de la vision agile du produit ? Normalement dans les projets agiles, on livre régulièrement de petits incréments. On a donc réfléchi avec ce leitmotiv en tête et avons proposé à tous les acteurs de notre produit nos options évaluées précédemment. Une seule de ces options a été validée par l’ensemble des acteurs : la vision échelonnée de la refonte. Passer d’une vision “big bang” à une vision progressive des choses a été pour nous un grand virage stratégique qui nous a permis d’apporter de la valeur régulièrement et plus facilement. Le nouveau départ: Lors de notre sprint de “remise en question”, nous avons établi notre nouvelle stratégie Apporter de la valeur: Avoir une vraie stratégie produit: "
    }, {
    "id": 5,
    "url": "http://0.0.0.0:4000/les-microservices-cest-pas-automatique/",
    "title": "Les microservices, c'est pas automatique !",
    "body": "2022/03/06 - Un effet de mode: Rentrons rapidement dans le vif du sujet. J’ai moi-même participé à cette hype il y a quelques années, avec très certainement un manque de recul et un engouement démesuré pour une manière de faire qui semblait nouvelle et révolutionnaire. Dans le monde du Java qui est le mien, Netflix avait pavé la route en mettant à disposition tout un tas d’outils promettants monts et merveilles, et Spring Boot - déjà très à la mode aussi - rendait la chose encore plus simple. Enormément d’entreprises (et donc de développeurs) se sont engouffrés dans cette brèche, pour déchanter quelques années plus tard. Cet article expliquera, de la manière la plus factuelle possible, l’ensemble des choses qu’il faut comprendre sur les microservices pour pouvoir décider, en toute connaissance de cause, s’il est pertinent de s’y lancer ou non. Aujourd’hui, l’effet de mode sur ce sujet s’est bien dissipé (bien qu’encore assez présent) et même si la technologie autour de cette pratique a bien évoluée, il n’en reste pas moins que c’est un chemin complexe et souvent anti-productif. Pour les plus avertis d’entre-vous, vous trouverez très certainement des raccourcis dans mon discours, ou des choses qui ne sont pas entièrement expliquées. Cela est voulu et assumé pour que ce billet reste digeste et ne se transforme pas en livre blanc sur le sujet. Si des zones d’ombre vous intéressent particulièrement, nous pourrons faire un article dédié à leur sujet. Les principales problématiques des logiciels: Ci-dessous, une présentation de 4 difficultés récurrentes dans le monde du logiciel. La liste n’est pas exhaustive, mais contient celles qui me semblent être les plus représentatives des problématiques du moment. La complexité: Le plus gros problème que l’on rencontre est, à mon sens, la complexité. Avec la montée en puissance du hardware (un téléphone est maintenant plus puissant qu’un ordinateur d’il y a 10 ans, voire même 5 ans), le nombre de logiciels qui explose, l’impatience croissante des consommateurs (tout le monde veut tout, tout de suite), il en résulte mécaniquement que les logiciels que l’on produit doivent être de plus en plus complexes. On explique la complexité par le fait que l’on veut faire mieux que ses concurrents ; elle justifie d’écrire plus de lignes de code ; elle nous donne le sentiment d’avoir bien travaillé et réussi à faire ce que d’autres ne sont même pas capable de comprendre. Bref, la complexité se développe dans nos applications, et sans trop de surprise, le retour de bâton ne tarde en général jamais à arriver : temps de développement qui explose, montée en compétence compliquée des nouveaux, relecture hasardeuse et pénible de son propre code, etc. Si seulement il existait un moyen de réduire cette grande complexité en un ensemble de petits problèmes plus simples… La performance: Lorsque l’on met une application en production et qu’elle fonctionne bien, la question de la scalabilité se pose : comment faire pour servir plus de traffic ? Il y a 3 réponses principales :  la scalabilité verticale : on augmente la capacité des machines (ajout de ressources) la scalabilité horizontale : on déploie de nouvelles instances l’optimisation : on optimise le code, pour que l’application tourne mieux (oui, oui, c’est une vraie méthode, bien que souvent oubliée / ignorée)La scalabilité verticale a ses limites, car un serveur très performant coûte rapidement très cher. L’optimisation aussi a ses limites, car le temps investi devient à un moment bien supérieur au gain effectif. La scalabilité horizontale est un bon compromis, mais elle implique quand même quelques choses à ne pas négliger (discutées plus tard dans la suite de ce billet). Un autre aspect de la performance qu’il ne faut pas oublier, ce sont les environnements de dev (donc des développeurs). Travailler sur une application trop grosse peut devenir compliqué pour le quotidien d’un dev, qui verra sa machine régulièrement à court de ressources. Tout le monde ne peut pas se payer tous les 2 ans des machines à plus de 2 000 euros, sous prétexte que ça rame. Existe-t-il un moyen de découper une application en plusieurs petites qui consommeraient moins ? Le modèle de données: Vous le savez bien, quand on commence à travailler sur un logiciel, on choisi en premier… la base de données (ok, il y a de l’ironie dans cette phrase, mais elle est malheureusement très proche de la réalité). Et cette base de données devient le coeur névralgique de l’application. Elle grossit de plus en plus, on y ajoute des tables/documents/clés et des colonnes/propriétés/valeurs et bien sûr tout le monde vient chercher dedans les informations qui l’intéressent. On se retrouve rapidement avec un système qui a grossi et on ne sait même plus qui a besoin de quoi. Cela se traduit le plus souvent par une peur voire une incapacité à faire évoluer le modèle et à le nettoyer (on ne veut pas casser une requête qui pourrait exister sur cette donnée). Au final, un modèle de données qui se dégrade de plus en plus (on ajoute, on ne supprime pas) et un couplage de plus en plus fort et de moins en moins maîtrisé. Comment faire pour rester maître de ses données ? Les technologies: La technologie évolue vite, plus vite que nos logiciels. Alors à un moment, il faut se lancer et faire des mises à jour. Quand je dis “à un moment”, ce n’est pas forcément dans la minute, dans l’année, ou même dans les 5 ans. Tout cela est une gestion du risque, mais c’est un autre sujet. Par contre, c’est sûr qu’à un moment il faudra évoluer. Le problème, c’est qu’en général, il faut faire évoluer toute sa base de code en même temps car… c’est le même logiciel. Et ça…. ça pique un peu, voire beaucoup. Pire, si au lieu de faire une “simple” montée de version, on décide de changer complètement de langage ou de paradigme ? Là c’est la cata. Des semaines, mois, années de boulot pour tout réécrire, tout en maintenant l’ancien logiciel en parallèle (car oui, il rapporte quand même des sous en attendant). Et la cerise sur le gâteau, c’est quand le nouveau logiciel met tellement de temps à sortir, qu’il faut continuer à faire évoluer l’ancien pendant ce temps. Si vous avez quelques années d’expérience et vu 2 ou 3 sociétés différentes, il y a de grandes chances pour que cela vous parle tellement c’est fréquent. Et si l’on ne migre pas ? Alors on choisi souvent entre subir des failles de sécurité, des difficultés à recruter, ou un manque de documentation sur le net. Alors, comment ne pas subir la techno, et rester en capacité d’en changer lorsque c’est nécessaire ? Les microservices à la rescousse !: “Le code est pourri, on y comprend rien !”, faisons des microservices ! “Mon appli consomme 8 Go de ram, je ne peux pas la scaler”, faisons des microservices ! “J’en ai marre que les gens tapent directement dans nos bases. On va leur faire une API”, faisons des microservices ! “C’est chiant de ne pas pouvoir faire de Kotlin”, faisons des microservices ! Donc si l’on veut :  réduire la complexité en plusieurs petits problèmes plus simples, avoir des applications qui consomment moins et qui nous permettent de scaler uniquement le nécessaire, rester maître de ses données, pouvoir faire évoluer simplement les technologies utilisées,alors les microservices semblent être le Graal. Et sur le papier, c’est très beau ! Un ensemble de services autonomes qui discutent sagement entre eux, qui préviennent rapidement en cas de problème, qui ne font pas écrouler tout le système lors d’un bug, qui ne consomment que le strict nécessaire, qui sont gérés par des équipes différentes (mais qui discutent bien entre-elles), etc. La liste des bienfaits [marketing ?] des microservices est longue. Le revers de la médaille: Vous vous en doutez (et le ton ironique que j’emploie sciemment est un indice), ce n’est pas non plus le monde des Bisounours. Si l’on regarde de plus près, pour que cela fonctionne, il y a beaucoup de choses à prendre en compte. Le découpage en microservices: Sûrement le sujet le plus complexe et là où la majorité des équipes se plantent. Comment découper un affreux monolithe en jolis microservices ? La phase de découpageToute bonne migration commence par de la réflexion. La question primordiale à se poser est la suivante : comment et quoi découper ? C’est cette question qui est la plus complexe à ce stade. Un découpage trop fin augmentera le nombre de microservices nécessaires et donc toute la complexité associée. Un découpage trop approximatif va entraîner une dépendance forte (couplage) entre les microservices, annihilant leurs effets bénéfiques. Il existe des techniques pour faire un bon découpage et si vous ne les connaissez pas, alors c’est déjà une première alerte rouge que les microservices ne sont pas la bonne solution. La phase de transitionDans un monolithe, les appels de code se font de manière local (i. e. dans le même process). Par exemple, pour les langages à base de machine virtuelle, les appels entre les méthodes se font dans la même instance de VM. Or si l’on veut découper cela en plusieurs applications, il faut changer cela et passer par un autre protocole. En général, on favorise les APIs Web (JSON par exemple), ou les files de messages. Il y a donc une première phase de modification de l’existant, pour transformer bon nombre d’appels ad-hoc en un échange de données entre 2 parties. Au mieux l’infrastructure permettant cela est déjà en place, au pire il faut ajouter de nouveaux composants dans le système (le messaging par exemple). La phase de migrationUne fois le périmètre des services défini et le découplage du code effectué, on peut migrer une partie du code dans un service externe et autonome : un microservice. Les premiers sont en général simples à faire et c’est là que la chose est insidieuse : ce n’est qu’une fois bien avancé, que l’on se rend compte des erreurs faites sur la route et le coût de rectification est en général élevé. Les principales choses auxquelles penser sont décrites ci-dessous. L’isolation des données: Le premier problème que l’on rencontre est en général l’isolation des données. Pour que le service soit autonome, il faut qu’il soit seul maitre de ses données. Cela implique que d’autres parties de l’application ne doivent pas pouvoir lire ou modifier ses données, mais aussi bien entendu des systèmes externes (une requête manuelle ou automatique faite par une personne extérieure à l’équipe est considérée comme un système externe). Si ce n’est pas le cas, alors comment être sûr que la modification d’un format de données ne va pas avoir un impact latent sur un autre système ? Il faut donc exposer la donnée autrement (là encore, en général via le protocole HTTP ou le messaging) et définir un contrat entre notre service et l’extérieur. Notre modèle de données peut alors évoluer à sa guise, en fonction des besoins, tant que le contrat défini n’est pas rompu. C’est à ce niveau qu’est le second challenge : au lieu d’avoir un couplage local (en général validé par le compilateur), on se retrouve à devoir maintenir des contrats avec l’extérieur, donc à devoir les valider et les tester en permanence. Cela se fait via des outils à mettre en place et des tests supplémentaires à écrire. Le déploiement des microservices: Lorsque l’on a 1 ou 2 ou 5 microservices, rien de très compliqué pour les déployer. En plus, si le travail est bien fait, ils sont indépendants et donc ne nécessitent pas une orchestration de la mise en production (synchronisation entre plusieurs MEP pour ne rien casser). Cependant, au bout d’un moment, leur nombre augmente et mécaniquement plusieurs autres choses aussi, comme par exemple :  le temps de build global (checkout, compilation, tests divers, packaging, déploiement) les resources utilisées (dans la majorité des technos, chaque microservice a un coût d’entrée en termes de ressources) le nombre de personnes nécessaire pour les opérer la latence (voir ci-dessous)L’adage “diviser pour mieux régner” implique ici de déplacer un problème de couplage de code (problème de dev) sur une problématique d’ops (le déploiement). Et donc, partir dans du microservice sans une culture ops un minimum solide est en général un suicide à petit feu. Et je ne parle même pas ici du monitoring de tous ces services, ou du debug dans un environnement distribué… La latence: Mettons tout de suite de côté le contre-argument que les systèmes peuvent être orientés événements (event-driven) ou bien asynchrones (oui, c’est bien 2 choses différentes). C’est vrai et dans ce cas la latence est un problème bien moindre. Mais ce n’est pas encore comme cela que la majorité des systèmes sont faits. La majorité des systèmes sont en majorité fondamentalement synchrones et vont voir leur latence exploser lors d’un passage sur une architecture microservice. C’est assez simple à s’en rendre compte (quoi qu’il faille y penser !) : si au lieu de faire des appels au sein d’une même application (donc latence très faible, de l’ordre de la nano-seconde), vous le faites via un protocole comme HTTP (même avec des optimisations comme du protobuf ou parquet), on passe rapidement sur un ordre de grandeur de la milliseconde… au mieux. Rajoutez une couche de SSL entre vos services et c’est encore une poignée de millisecondes de perdues à chaque appel. Ajoutez de l’authentification (déchiffrage de token par exemple) et rebelote. Alors oui, on peut mettre en place une architecture pour éviter ces 2 choses là, mais on le fait généralement une fois que l’on a eu ces problèmes et surtout si, encore une fois, on a une maturité d’ops suffisante pour gérer cela. Une nouvelle fois, soit la latence augmente, soit on déporte le problème sur les ops, là où il n’y en avait pas avant. Le monitoring: Dans tout système, on doit pouvoir savoir ce qu’il s’est passé quand quelque chose ne va pas. C’est à cela que sert la majorité du monitoring. “Suivre les chiffres” permet surtout de s’assurer que tout va bien. Monitorer correctement une application n’est pas si simple. Alors en monitorer 10, 20, 50… Il ne s’agit ici plus de monitorer uniquement ses microservices, mais de monitorer les interactions entre tous les microservices : savoir qui discute avec qui, comment, pourquoi. Et si quelque chose ne tourne pas rond, remonter le fil de service en service jusqu’à trouver le coupable. Et ça… c’est compliqué dans un monde distribué. Plus de stacktrace pour vous montrer par quelles méthodes le code est passé. Il faut retrouver les appels HTTP avec leur payload, ou les messages échangés. Bien entendu, il existe là encore des outils pour faire cela. Par exemple, OpenTracing. Et c’est encore une fois quelque chose que nous n’avions pas besoin de faire avant, donc du temps, de l’énergie, de la maintenance en plus. La jungle des technos: Pour terminer cette partie, il faut parler de la technologie. Comme évoqué au début de ce post, la technologie va vite et les besoins et la hype autour de certaines choses sont inéluctables. La question n’est pas de savoir si on va devoir évoluer, mais quand. Les microservices, en isolant les bases de code de chaque service, rendent très simple de développer chaque service dans une technologie différente. Il est vrai que lorsque l’on veut faire quelque chose de nouveau, il faut se demander quelle techno ou quel paradigme sont les plus adaptés au besoin. Mais bien souvent on dérape vite et les microservices se transforment en un gigantesque bac à sable de test de techno. “C’est juste un microservice parmi d’autres, au pire si ça ne fonctionne pas on le refait !”. Bah… non. Avoir un ensemble hétérogène de technologies et de pratiques dans une entreprise rend compliqué l’apprentissage et la mobilité interne. Je ne dis pas qu’il faut l’éviter à tout prix. Je dis simplement qu’en général cela se fait sans vraiment de contrôle ou de stratégie et les microservices réduisent encore un peu plus les garde-fous sur ces sujets, car les échanges entre services se font sur la base de contrats, indépendamment de la techno utilisée. Et contrairement à ce que l’on pense, refaire un microservice (qui n’est pas forcément un nanoservice) dans une autre techno, n’est pas forcément trivial. Est-ce que les microservices sont un anti-pattern d’architecture: Ou dit autrement, faut-il les éviter ? La réponse est claire : Non. L’architecture microservice n’est pas un mauvais choix d’architecture en tant que tel. Le mauvais choix serait de s’y lancer sans bien en comprendre les tenants et aboutissants. Elle règle un certain nombre de problèmes, mais en crée d’autres. Comme très souvent en informatique, il s’agit d’un choix à faire (trade-off en anglais). Le ton ironique utilisé à de multiples reprises vise à mettre l’accent sur des choses que l’on pense naïvement bonnes au premier abord (et j’ai commencé par avouer y être moi-même tombé il y a quelques années), mais qui ont des conséquences importantes. J’espère au moins que cela vous fera réfléchir avant de vous y lancer. Alors, quand faire du microservice ? Déjà, ce n’est pas forcément un choix binaire. Ce n’est pas soit TOUT en microservices, soit RIEN. On peut très bien faire 2 ou 3 microservices qui cohabitent avec un monolithe (mais un monolithe bien foutu !). Si l’on a besoin de scaler une petite partie de l’application, d’utiliser une techno particulière pour des raisons techniques, juridiques, business, etc. , ou bien de séparer la base de code du reste pour une bonne raison, alors l’architecture microservice est peut-être adaptée. Dans ce cas, il reste à se poser la question suivante : Est-ce que les bénéfices seront supérieurs au coût investi ? Nous avons vu plus haut que pour fonctionner correctement, un architecture microservice a besoin de plusieurs choses :  un découpage correct (= du temps) une isolation des données (= de l’expertise dev) un déploiement rôdé (= de l’expertise ops) de monitoring (= de l’expertise ops) d’infrastructure (load balancing, messaging, tracing, resillience, orchestrateur, etc. )Alors oui, si votre maturité sur ces sujets est assez élevée, vous en tirerez sûrement des bénéfices. Si non, ce n’est pas grave, il y a d’autres solutions. Une autre solution ?: Si vous pensiez échapper au fameux terme de DDD (Domain-Driven Design), alors vous avez eu tord :) Au début de l’article, j’indique que la complexité est selon moi le plus gros problème des applications. Les autres problèmes ajoutent de l’huile sur le feu, mais ne me semblent pas aussi handicapants. Beaucoup d’équipes se sont tournées vers les microservices en pensant réduire leur complexité métier en découpant leur logique en plusieurs services. L’idée est bonne, mais il y a 2 choses à ne pas oublier : savoir découper correctement et accepter une augmentation de la complexité d’infrastructure en échange. Et c’est là que beaucoup d’entreprises déchantent, car elles n’avaient pas prévu ce palier. Une autre solution est donc… de rester sur un monolithe. Oui, en 2022, on peut encore faire des monolithes ! Mais pas n’importe comment. Un monolithe bien pensé, sans toute la complexité d’infrastructure qu’amènent les microservices. Et pour cela, quoi de mieux que d’utiliser le DDD. Car oui, on en revient toujours à ça. La complexité. Comment la réduire ? Bien comprendre son domaine et créer des sous-domaines adaptés. L’important n’est pas comment ils communiquent (par API Web, messages, RPC, etc. ), mais bien de comment ils sont découpés. On peut donc parfaitement les mettre dans un monolithe et il sera toujours temps de créer des microservices le moment venu, où non seulement nous serons prêt à traiter la complexité qu’ils apportent, mais aussi et surtout lorsque nous aurons de vrais problèmes que les microservices peuvent adresser. En gros, quand vous vous lancez dans un nouveau projet, ne partez pas en microservices par défaut sous prétexte qu’“on est en 2022 quand même !”. Conclusion: Pour finir cet article qui était plus long que prévu (et qui pourrait l’être bien plus), retenez une chose : les microservices coûtent cher, en terme de temps, d’argent et de personnes. Ce n’est pas un choix idiot, mais ça ne doit pas être un choix par défaut ou par ignorance. Ok, c’est à la mode. Et alors ?! Si vous voulez faire des microservices, commencez par bien comprendre et maitriser votre métier au travers de la pratique du DDD. On ne peut pas faire de microservices correctement sans avoir une vision claire des frontières entre les services. C’est une tâche complexe qui nécessite de l’expertise. Par contre, on peut faire du DDD sans forcément faire de microservices. Et il ne faut pas en avoir honte. A bon entendeur… "
    }, {
    "id": 6,
    "url": "http://0.0.0.0:4000/carriere-de-jeune-developpeur/",
    "title": "Ma carrière de jeune développeur",
    "body": "2022/02/23 - Cela fait maintenant 5 ans que j’ai terminé mes études et pourtant je n’ai eu de cesse de revoir ma vision de ce qu’est un développeur. Voici une rétrospective du premier changement de cette vision. Pourquoi devenir développeur ?: Je pense que j’ai eu de la chance dans mes études, j’ai eu des professeurs qui ont su me transmettre leur savoir et me guider afin que je puisse trouver ce qui me plaît. En effet j’étais très indécis sur ce que je voulais faire à l’époque. J’avais déjà créé des infrastructures réseaux à plusieurs reprises, c’était quelque chose de connu et de confortable pour moi, mais j’avais envie de nouveauté. Dans le développement, il y avait un côté créatif qui me plaisait beaucoup, une exploration du métier intéressante, avec pour point de convergence la programmation orientée objet (POO). Une notion très abstraite au premier abord, mais qui démontre toute sa puissance quand on la maîtrise. J’ai vraiment adoré ces notions d’objets vivants qui représentent quelque chose dans le monde réel. Comment en leur envoyant des commandes on les fait vivre, et comment ils nous rendent un service. J’avais alors hâte de voir comment j’allais pouvoir utiliser tout ça dans un vrai cas métier. C’est une notion qui m’était chère et je ne l’ai retrouvée que bien plus tard quand j’ai lu Elegant Objects de Yegor Bugayenko. Premier saut sans parachute dans un projet et premières erreurs: Mon premier travail (et mon dernier stage) a été un moment charnière dans ma vie de jeune développeur. C’est sûrement à ce moment-là que je me suis le plus construit professionnellement. Je m’en souviens comme une période bénie où des développeurs professionnels m’ont appris comment développer et surtout comment me comporter. Mon premier projet a démarré comme pour beaucoup de développeurs en ESN par la réception d’un joli cahier des charges qui devait me permettre de me lancer en autonomie (seul) dans la réalisation de mon tout premier logiciel (un ERP). Je dois dire que ce moment de lecture a été confortable, car ça me rappelait les énoncés des projets/examens que j’avais eu lors de mes études. Une consigne claire et orientée solution qui ne laisse pas beaucoup de place à la créativité… J’ai naturellement demandé de l’aide aux autres développeurs de l’entreprise qui m’ont expliqué patiemment pleins de choses. J’étais bien loin d’imaginer que j’allais jeter cette notion d’objets vivants que j’avais vu en cours au profit d’un objet qui n’est rien de plus qu’un tas de donnée que je faisais seulement transiter au sein de services…. Cela m’a fait un petit choc, j’ai eu l’impression de devoir apprendre plein de choses et de ne pas être prêt professionnellement. Je me sentais obligé de faire mes preuves, sans savoir comment faire. On m’expliquait qu’une classe est la représentation d’une table en base de données, qu’on la fait transiter au sein de services métiers qui s’appellent les uns les autres. Cela me semblait alors logique et plein de bon sens. C’était aussi plus facile à faire et sans grosse complexité comparé à ce que j’avais appris en cours. Facile au début oui, mais dès que le projet a commencé à grossir j’ai perdu régulièrement le fil des méthodes qui étaient appelées et je devais recommencer de zéro pour m’y retrouver. On m’a aussi donné un projet exemple sur lequel je pouvais m’appuyer pour comprendre comment je devais développer. Autant vous dire que j’ai reproduit toutes les erreurs qu’il y avait déjà dans ce projet et que j’y ai ajouté les miennes. J’étais bien incapable de remettre en question ce projet exemple et encore plus ma manière de développer. J’étais relativement seul à développer et j’ai eu peu d’aide externe sauf quand j’osais la demander. Voici le résultat de ce qui a été produit :  Un logiciel orienté CRUD thinking / DATA et pas métier Aucun test unitaire (à l’époque, je ne me doutais pas à quel point ces derniers m’auraient fait gagner du temps et de la sérénité…) Des méthodes de plusieurs centaines de lignes gérant des “trucs” Des jours entiers pour débuguer des entités qui mutaient dans l’ORM sans que je sache pourquoi Des triggers Base De Données qui contenaient de la logique métier et qui avaient été développés sans que je le sache Et bien d’autres chosesAujourd’hui je pense que je trouverais “effrayant” ce que j’ai développé. Même s’il y avait un projet exemple qui m’a fortement guidé dans mes erreurs je pense que j’en ai fait plusieurs qui sont entièrement ma faute :  Ne pas avoir cherché à comprendre le besoin exprimé, mais uniquement à produire du code qui marche (toujours comprendre le besoin avant de faire des développements !) Ne pas demander de l’aide alors que j’en avais besoin Ne pas remettre en question l’existant. Parce que oui même avec plusieurs années d’expérience on continue de faire plein d’erreurs Ne pas m’intéresser au métier, mais uniquement au cahier des charges si réconfortant sur le moment (il était vraiment bien écrit, je n’en ai jamais revu d’aussi bonne qualité) Faire des erreurs pour rester dans les “temps” qu’une autre personne avait déterminé à ma place… Qui n’a jamais entendu “J’ai vendu cette fonctionnalité 2 jours donc tu dois la faire en 2 jours” ?Finalement et malgré tout ce que j’ai dit, je garde un très bon souvenir de ce projet. Il s’est bien passé et le client a été super content ! Devenir un professionnel: Une fois ce premier saut réussi j’ai eu la chance d’avoir un nouveau projet ainsi qu’un nouveau manager. Le contexte était par contre beaucoup plus complexe. Les faux pas pouvaient coûter cher et il était hors de question de se planter avec ce client qui était critique pour l’entreprise. J’ai commencé à participer à des réunions avec le client, on me demandait mon avis, j’ai dû écrire des spécifications, rassurer le client quand ça n’allait pas, résoudre des bugs en direct. Tout un tas de choses que je n’avais jamais fait auparavant en ayant juste mon joli cahier des charges. C’est ici que ma vision de ce qu’est un développeur a changé pour la première fois. On n’est pas un développeur, on est un professionnel du développement. Savoir écrire du code ne fait pas de nous un professionnel. Ce changement, je le dois essentiellement à mon manager de l’époque qui m’a accordé très facilement sa confiance et a sû me donner les clés nécessaires pour me professionnaliser. Malgré son côté ESN cette entreprise a toujours été bienveillante en cherchant à faire grandir ses collaborateurs, même les plus inexpérimentés. Avec le recul, je pense que c’est la meilleure ESN dans laquelle puisse espérer tomber un développeur. Mais du coup, qu’est-ce qui fait de nous des professionnels ? Je dirais avant toute chose que c’est un état d’esprit. La volonté d’apporter de la valeur au produit, à trouver des solutions pertinentes en prenant en compte le contexte actuel. Cela n’est pas facile et ça l’est encore moins quand on est inexpérimenté. Voici les lignes directrices qui m’ont le plus aidé à acquérir cet état d’esprit :  Justifier ses choix quand on nous le demande S’engager à fond dans les développements Dire “non” quand les demandes sont déraisonnables Ne pas se trouver des excuses pour ne pas faire Gérer le temps et les priorités Ne pas céder à la pression Travailler en équipe, demander de l’aide et proposer la sienneSans le savoir j’appliquais beaucoup de conseils du livre Clean Coder de Bob Martin. PRIMOBOX : L’entreprise qui m’a fait grandir: J’ai fini par partir de cette ESN pour tout un tas de raisons qui me sont propres. C’est comme ça que je suis arrivé chez Primobox et ça été une révélation pour moi. J’y ai perpétué ce que j’avais appris, désappris d’autres choses, évolué et surtout j’y suis devenu ce que je suis aujourd’hui. Un développeur passionné qui sait qu’il lui reste encore bien des choses à apprendre. Cela je vous en parlerai dans un prochain article. ConclusionL’école m’a appris les bases du développement, mais c’est en entreprise que j’ai compris ce que voulait dire être un professionnel du développement. L’école ne nous prépare pas aux enjeux du monde du travail, à comprendre les interactions humaines et à gérer les pressions externes. Tout cela s’acquiert avec l’expérience, et avec de la chance, des collègues seront là pour nous aider. "
    }, {
    "id": 7,
    "url": "http://0.0.0.0:4000/le-messager/",
    "title": "Le “Messager” : un projet pas comme les autres chez Primobox",
    "body": "2022/02/02 - Trouver une entreprise dans laquelle on peut progresser, apprendre, innover, tout en forgeant un logiciel de qualité et des compétences partageables à tout le monde, n’est pas chose courante. Primobox est de cette trempe là. Laissez-moi vous raconter brièvement cette aventure que j’ai eu la chance de vivre. Genèse: Vous connaissez l’adage “Diviser pour mieux régner” ? C’est ce que nous invite à faire l’approche DDD (Domain Driven Design). Et c’est avec ce principe en tête (délimiter des Contextes Explicites et Discrets 1) qu’un projet de Messager autonome est né chez Primobox (pour faire simple, le Messager est celui chargé de remettre le courrier à ses destinataires). Doté d’une grande responsabilité, simple au premier abord mais qui permet d’envisager des options et des possibilités multiples, en toute autonomie. Une fois ce domaine (contexte) métier spécifique identifié , parmi les autres besoins clients que doivent satisfaire les solutions proposées par Primobox, il était plus facile de lancer un chantier ciblé de modernisation du code. Avec un périmètre et des relations bien définis, ce véritable “métier dans le métier” allait pouvoir prendre son envol. Vision: Un projet réussi commence par une vision. Alexandre Fillatre, CTO chez Primobox a su initier et nourrir cette vision. Basée sur une idée simple : les différentes phases qui composent l’activité de l’offre de service chez Primobox (dématérialisation de la relation entre employés et employeurs) ont chacune une raison d’exister propre, tout en étant capable d’interagir avec les autres parties et d’évoluer de manière autonome. Alexandre nous a partagé cette vision et nous a fait confiance pour la mettre en œuvre. Utilisateurs: En amont, un vrai travail pour comprendre le besoin utilisateur avait été initié et je l’ai traduit en éléments que nous allions mettre dans le cœur du “Messager”. L’atelier de Wording et les User Journeys m’ont été très utiles pour cerner le domaine métier dans son ensemble et dans ses particularités. Clarté: La clarté se gagne par l’analyse. Grâce aux travaux préliminaires de l’équipe Produit et Ergonomie, j’ai pu coucher sur le papier une carte (à la Wardley) des interactions entre le Messager et les autres éléments qui constituent la solution globale dont nos clients ont besoin. Cela permet de savoir quelles fonctionnalités sont plus importantes, lesquelles sont annexes, lesquelles sont laissées à la responsabilité d’autres modules (existants ou à créer) voire externalisées. Des choix stratégiques importants sur le périmètre fonctionnel ont été faits afin de réduire la charge de travail et la complexité présente sur certains produits. Cela dessine une forme d’architecture, plus basée sur les besoins que la technique. Cela nous libère aussi des détails techniques, car sur un Context Mapping (toujours cette notion de cartographie) se dessinent les frontières entre les Bounded Contexts; et l’on voit facilement les choix contraints par l’existant (legacy), et par ailleurs les espaces où les contraintes changent de nature. Par exemple, là où la performance est plus importante que la compatibilité. Audace: Prendre des paris audacieux, après tout, pourquoi pas ? Pourquoi rester conforme à ce qui a été fait par le passé et se priver de progresser ? Parlons du langage de développement. Primobox a un historique Java solidement ancré. Moi j’arrivais du monde . Net. Alexandre Fillatre a su discerner qu’un bon développeur ne saurait s’arrêter à une syntaxe donnée, l’essentiel étant ce que l’on sait obtenir de tel ou tel langage de développement. La POO est une chose universelle mais les langages modernes évoluent pour être plus hydrides et flirtent plus facilement avec la Programmation Fonctionnelle. C’est le cas de C#, puis de Java qui lui a emboîté le pas. Quitte à s’éloigner de C# 9 pour aborder la JVM, autant opter pour un langage du même niveau. C’est là que nous avons pensé à … Kotlin ! Etant pour ma part un adepte précoce de C# et de l’écosystème . Net, je n’avais jamais eu à me frotter au monde Java ailleurs que dans mes études. Grâce à Kotlin, franchir le cap a été pour moi d’une facilité déconcertante. J’ai retrouvé tous mes réflexes acquis en C# avec une syntaxe encore plus élégante. Ayant l’habitude de Linq, Kotlin est naturellement provisionné pour faire la même chose. En mieux ! Et il est facile de trouver de l’aide et des exemples. Et des très bons tutos. Arrière boutique: Ah oui, si vous cherchez des ressources sur Kotlin dans les Internets, vous verrez que ce langage est la panacée pour les applications mobiles. Et pourtant ! C’est excessivement réducteur. Kotlin produit (aussi) du bytecode qui va tourner sur la JVM. Donc c’est un parfait candidat pour tout le code backend. C’est ce que nous avons fait pour cette application complexe, avec une architecture très évoluée, ce qui nous a permis d’être parfaitement intégré dans l’existant Java et surtout très performant. Idiomes: Quitte à choisir un langage qui nous était inconnu (à moi et mon coreligionnaire, pur Javaiste pour sa part) jusqu’ici, autant le faire bien et complètement. Kotlin a de multiples talents. Facile d’apprentissage, il emprunte un peu à Typescript, C#, Scala et bien sûr Java, puisqu’il produit du bytecode ciblé pour la JVM. Son éditeur (Jetbrains) a tout fait pour que les développeurs Java s’y retrouvent, et puissent même ré-utiliser n’importe quelle classe du JDK ou n’importe quelle lib Java. Et ça fonctionne très bien. On peut même écrire du code “façon Java” (utiliser des exceptions, mettre des if en pagaille, des null partout…). Bref, si on n’y prenait garde, on écrirait presque du Kotlin qui ressemble à du “mauvais” Java (je veux dire par là du Java d’avant guerre, avant la version 8 si vous préférez…). Pourtant avec Kotlin, tout est fait pour écrire du code “élégant”. J’en veux pour preuve la non nullabilité des types par défaut, l’immutabilité par défaut, la présence des data classes (équivalent des Records de C# ou Java 16) parmi tant de choses qui m’ont enchantées. On notera aussi que Kotlin dispose nativement de structures de données avancées (Pair, Triple, Linked List, Tree, etc…) et de possibilités de manipulation de fonctions qui sont juste délicieuses. Et j’allais oublier de mentionner les très puissantes co-routines. Attention: Tout au long de notre travail, une attention particulière a été portée aux autres membres de l’équipe. Nous avons communiqué régulièrement :  sur l’avancée du projet, même si nous n’avions pas de revue de sprint (on en parle plus loin), sur les innovations ou techniques que nous avions choisi de mettre en œuvre, car celles-ci pouvaient aussi trouver leur place dans d’autres projets du groupe. Nous sommes aussi dans une démarche d’invitation permanente : tous les autres membres de la R&amp;D peuvent venir collaborer au projet, apprendre Kotlin ou tout autre concept que nous avons mis en œuvre. Langage Naturel: Le langage des utilisateurs et des experts métier a été le nôtre tout le long de la réalisation. Pas de jargon de développeurs. Notre domaine métier est en français. Toute la modélisation et le codage du logiciel se sont donc faits dans cette belle langue. Nous avons réservé l’anglais pour les parties purement techniques (logées dans des composants isolés et accessibles via les Ports &amp; Adaptateurs, conformément à l’architecture hexagonale exposée plus loin); par exemple la mécanique d’accès à la base de données ou au bus de message. C’est très pratique pour se rendre compte au premier coup d’œil si notre métier (en français) reste pur et ne se mélange pas avec des considérations techniques (en anglais). Stratégie: L’approche DDD est un guide dans la nuit. Ce n’est pas à proprement parler une méthode, et surtout pas un framework. C’est un ensemble d’outils et de bonnes pratiques, tout à fait compatible avec l’agilité et le software craftsmanship. Nous avons eu cette approche d’emblée, dès les premières heures du projet. C’est, il me semble, la condition sine qua non pour réussir.  Comme dit précédemment, j’ai commencé par la mise en place d’une Wardley Map, puis avec Damien Boué nous avons travaillé sur des Bounded Contexts Maps. Cela nous a permis de nous synchroniser aussi avec les équipes Produit. Tactique: En DDD la stratégie s’accompagne toujours d’une tactique, je ne vais pas vous faire ici un cours de DDD, les ressources sur Internet ne manquent pas. Je ne vais pas vous parler ici des essentielles décompositions en agrégats, entités et value objects (chasser la Primitive Obsession). Je vous dirai seulement qu’une des leçons que nous avons apprises est de songer sérieusement à limiter la taille de nos agrégats ! (cf ces bons conseils peu connus de Vaughn Vernon) Jusqu’à ce qu’ils ne contiennent qu’une entité. Cette idée peut vous paraître saugrenue mais vous lui direz merci quand vous verrez la complexité accidentelle poindre son nez. Bien sûr cela demande à réfléchir plus et surtout à ne pas se laisser entraîner par une conception basée sur le Mapping Objet Relationnel (ORM ou Objet Documents Mapping dans le cas des entrepôts NoSQL) qui, entre autres inconvénients, pousse notre modèle à être anémique. Mais aussi lorsque l’exécution de nos adapters devient concurrente (comme dans toutes les API Web), nous aurait obligé à poser des verrous transactionnels (et donc faire effondrer la performance). Cela est inévitable lorsque les agrégats sont trop gros et que l’on veut y accéder en écriture. Donc, gardons les atomiques ! Coeur: L’une des (nombreuses) brillantes idées de ce projet a été de suivre le précepte de “Functional Core/ Imperative Shell”, introduit en 2012 dans la communauté Ruby, puis repris plus récemment par Kenneth Lange et aussi par Thoman Pierrain &amp; Bruno BOUCARD ; Cette approche a d’énormes avantages, citons-en 2 :  d’abord repousser les effets de bord (les fameux, ceux qui rendent vos tests trop compliqués, plus du tout isolés, voire inconsistants et cachent de nombreux bugs) en dehors des considérations métier.  corollaire : cela vous permet de vous concentrer sur la seule logique métier de manière fonctionnelle, c’est-à-dire en privilégiant l’immutabilité et la transparence référentielle. Cette façon de penser le logiciel en y plaçant un cœur purement fonctionnel (dans les 2 sens du terme) permet d’isoler vraiment les tests métiers et donc de se passer entièrement des mocks à cet endroit. Dans le cœur métier propre du Messager sont également apparues d’autres frontières, et donc d’autres Bounded Context pour des composants qui avaient besoin de pouvoir évoluer sans casser les autres. Tout ce travail d’isolation (via des ACL, anti corruption layers) est primordial pour ne pas s’enfermer dans une complexité accidentelle et dans ce qui devient inexorablement, malgré toute bonne volonté, une “big ball of mud” (grosse boule de boue). Hexagone: Il y a le cœur fonctionnel, et il y a la coquille (shell en Anglais). C’est ce que l’on retrouve aussi dans les architectures hexagonales (ou Clean Architecture). Dans la partie coquille (hors cœur métier donc) on va retrouver les ports et adaptateurs. C’est cette philosophie que nous avons appliquée fortement. Et nous avons fabriqué des adaptateurs techniques pour toute situation spécifique. L’absorption de commandes venues d’une API REST est un adaptateur indépendant. Il a une dépendance directe avec les objets métiers. Par contre pour toute opération en base de données (nous avons, en prévision d’un CQRS séparé les opérations de lecture et celles d’écriture), il y a des adaptateurs dédiés. Il en va de même pour les opérations d’envoi et de réception de messages dans un bus (afin de prévenir les agents dans d’autres Bounded Contexts qu’un événement important s’est passé dans notre domaine. Qualité: TDD offre ce double avantage d’être la méthode d’écriture du logiciel (et non de test) qui nous a permis de faire du design émergent guidé par le DDD tactique et de toujours se fixer des petits pas. Et bien sûr, on obtient une couverture de tests très satisfaisante, puisqu’aucune ligne de code ne devrait être écrite si elle n’est pas justifiée par un test. Kotlin (encore lui !) vient avec une géniale librairie : Kotest. Attention à ne pas faire comme moi au début, de faire la confusion entre KotlinTest et Kotest. C’est bien Kotest qui permet de choisir parmi moult styles de tests, il y en a vraiment pour tout le monde : style Behaviour Driven Development (celui que nous avons choisi), style Scala, Ruby, Cucumber ou JavaScript/TypeScript. Ou ce bon vieux JUnit. Écrire des tests, c’est bien joli, mais encore faut-il s’assurer qu’ils servent à quelque chose et qu’ils sont robustes. Les outils déjà en place pour les autres projets Java de l’entreprise (Jenkins, SonarQube, un scanner de vulnérabilité des dépendances) se sont parfaitement intégrés à notre nouveau projet Kotlin. La Couverture de code a pu donc être constamment mesurée, mais j’en dirai plus au prochain chapitre. D’autres outils sont venus renforcer cette recherche de qualité :  ArchUnit, pour vérifier en permanence le bon usage des dépendances et vérifier que nous ne cassions pas les principes de l’architecture hexagonale.  Gatling pour s’assurer que notre solution tient la charge en situation réelleRigueur: Un grand enjeu de la qualité logicielle semble être la couverture de code, mais cette mesure peut s’avérer erronée, comme je le prouve dans ma présentation sur les tests par mutation de code. Dans ce projet, il nous a paru opportun de consolider notre approche TDD, parce qu’après tout nous sommes tous faillibles, et qu’un outil qui nous montre que nous manquons de tests, que des bugs sont encore présents, est tout simplement une aubaine. Nous avons choisi la lib PiTest, qui fonctionne sur le ByteCode Java, et donc très bien avec Kotlin. Simple d’usage, facile à ajouter à un pipeline d’intégration continue, cet outil va nous aider à mieux coder et mieux tester (l’un ne va pas sans l’autre). Il nous a rappelé sans faillir aux règles du TDD (ne pas écrire une instruction qui ne soit justifiée par un test). Par contre, il est avisé d’utiliser ce genre d’outil sur du nouveau code plutôt que sur du legacy, et de cibler du code purement métier, c’est-à-dire au centre (functional core) de l’architecture hexagonale. Paradigme (changement de): Ce n’est pas chose facile que de bousculer (un peu) les habitudes des développeurs. Alors que la programmation orientée objet est maîtrisée par tous mes collègues, je leur ai fait la proposition de parier sur l’immutabilité. L’immutabilité, c’est quoi ? Une idée qui nous vient de la programmation fonctionnelle mais qui s’applique très bien à la POO. Le principe est simple : quand les objets ne “mutent” pas (comprendre : ne changent pas d’état) alors ils sont plus facile à maîtriser, à comprendre et surtout on évite beaucoup, beaucoup de bugs car on limite grandement les effets de bord dans les méthodes de ces objets immuables. Mieux encore, cette immutabilité est facile à obtenir avec des langages tels que Kotlin (celui que nous avons choisi pour ce projet) ou même Java (le mot clé final existe et il a été ensuite complété avec des librairies qui poussent une vraie immutabilité des objets, comme par exemple https://www. baeldung. com/immutables). Elle permet de transitionner naturellement vers une écriture de code plus “fonctionnelle” c’est-à-dire avec une meilleur répartition des responsabilités (voir chapitre suivant), et une plus grande intégrité référentielle. Solide: Notre attention s’est focalisée sur la production d’ un code SOLID. Ce n’est pas facile de vous parler des principes d’un code SOLID, surtout que cela fait partie de la grande famille du code “propre” (Clean Code). Le premier principe est peut être le plus important et le plus simple à comprendre : Simple Responsibility. Et tous les autres en découlent. Nous avons toujours cherché dans ce projet à exprimer et répartir les responsabilités de façon très claire et très cohérente entre les différents modules. Que ce soit dans le découpage métier (Bounded Contexts, DDD Stratégique) ou dans l’approche technique (Archi Héxagonale DDD Tactique), notre objectif a tout le temps été de limiter la responsabilité à un niveau minimal et acceptable, afin de mieux séparer et isoler ces responsabilités. Les cartes CRC nous ont aussi beaucoup aidé à y voir plus clair dans notre design. Découplé: Le découplage est un fondamental à la fois de l’architecture d’un logiciel mais aussi de la journée ordinaire d’un développeur. Il s’agit simplement de ne pas accumuler la complexité et les interactions douteuses qui peuvent se loger dans le code. Pour cela nous avons fait le choix d’abstractions pour définir et maîtriser ces fameuses dépendances. L’idée est d’exprimer simplement les choses en termes de “quoi” et non de “comment”. Exemple : au lieu de dire à notre code “je veux une base de données Mongo pour stocker des informations”, nous avons établi une abstraction (interface) qui dit “je veux pouvoir stocker cette information”. Le “comment” ne nous intéresse absolument pas. En se référant à cette abstraction (“Je_veux_stocker_information_X”), nous voilà libres de son implémentation. Et c’est double bénéfice. D’abord nous pouvons choisir librement l’implémentation et en changer à tout moment au cours de la vie du projet (nous avons choisi Mongo DB mais peut être que ce choix sera remis en cause). Et surtout, pendant les tests fonctionnels, nous n’avons pas l’obligation de mettre en route la fameuse base de données Mongo que nous avons choisie pour la production. Nous pouvons juste utiliser un fake, c’est-à-dire une implémentation naïve, simpliste. Evenementiel: Une autre grande force de ce projet est que nous avons d’emblée pris en compte la nature événementielle d’une application. Dans un logiciel, il se passe des choses. Il est plus important de capturer des évènements que d’enregistrer l’état des objets. Malheureusement, trop d’applications sont développées en mode CRUD (Create Read Update Delete), aidées (mais pas dans le bon sens) par les frameworks de mapping ORM. Ce mode de développement rend compliqué toute migration fonctionnelle, car justement il est difficile de placer des règles métiers quand on pense que tout se limite à des opérations d’ajout/modification/suppression/lecture en base de données. En pensant événements, on est au contraire ouvert à tout ce qui peut se passer comme vérifications, contraintes, évolution du métier. On est bien plus proche des actions des utilisateurs. On peut créer des programmes plus aptes à répondre au besoin métier, et moins coincé dans une solution technique. En pratique, nos agrégats métiers émettent des événements, en réaction à des commandes, après avoir opéré les vérifications métier qui s’imposent. Nous avons mis en place un mécanisme de remontée automatique des événements (event bubbling) depuis les entités filles qui composent un agrégat global (grâce à ReactiveX et ses Observables). Partage: Cela a été une discussion intéressante (parmi tant d’autres) : Comment s’y prendre pour se partager le travail et le partager avec les autres ? Bien sûr il y a l’usage de l’attirail Jira, Git, Jenkins… Mais ce qui importe c’est surtout la façon dont on s’en sert. Nous avons opté pour des ADR dans les points clé de notre repository, afin de garder trace de nos discussions et de remettre en contexte les choix tactiques opérés dans le code. Nous avons aussi mis l’accent sur le pair programming. Mais nous avions quand même voulu garder les Code Reviews, pour prendre de la hauteur sur le code produit (essentiellement par Damien Boué et moi même). Je suis plus partisan du Trunk Based Development, mais en adoptant des tests plus longs à exécuter comme le Mutation Testing, il était intéressant d’isoler tout nouveau code dans une branche dédiée afin de voir s’exécuter correctement toute la lignée de tests, sans compromettre la branche principale (on aurait pu faire du TCR mais c’est un peu violent). Nous avons plutôt opté pour des Short Lived Branch avec des règles du jeu : pas plus de 2 branches différentes en simultané et ne portant pas sur les mêmes “zones” du code. Au bout d’un moment, nous avions assez d’abstractions (branching by abstraction) pour éviter des “rebase” douloureux. Bien sûr, quand nous avions à faire des refactoring qui touchaient plusieurs endroits du code, il fallait d’abord solder toutes les branches en cours, avant de s’attaquer à des modifications qui auraient eu trop d’impacts sur l’architecture ou les interfaces fondatrices. Pour éviter que les refactoring ne durent trop longtemps et ne conduisent à des réconciliations de code houleuses, j’ai recommandé d’adopter la méthode Mikado, afin, une fois de plus, de progresser par petits pas, de merger très très souvent, de ne pas se lancer dans des travaux d’Hercule, et de se rendre compte des problèmes au plus tôt. Le refactoring devient également plus aisé en adoptant la méthode “parallel change” ou “expand and contract” : on ne casse pas l’existant; on implémente dans une nouvelle méthode ce qui doit être refactoré, et petit à petit on transite vers le nouveau code, avant d’effacer l’ancien quand il n’est plus utilisé nulle part. Encore une fois, l’architecture hexagonale, avec son découplage maximum, nous a permis de minimiser la casse. Je suis très content d’avoir travaillé comme cela, car les revues de code nous ont permis de se poser la question “est ce que notre design reste cohérent” à chaque étape de la construction du logiciel. Agilité: Il était important d’avoir du feedback rapide même sur un produit qui n’est pas encore mis en exploitation. Nous avons opté pour un travail en flux type Kanban, avec des jalons variables qui étaient matérialisés par des Minor Releases, selon nos désidérata et contraintes. Nous avons pu compter sur une Intégration Continue (CI/CD) super efficace grâce au soutien de l’équipe DevOps en place. Résultat: Le projet n’est pas encore terminé, et vous n’en verrez pas une démonstration graphique, car ce n’est qu’un composant au milieu de tant d’autres. Mais comme il va devenir un maillon fort de l’édifice logiciel de Primobox, c’est la fiabilité, la rapidité et la performance de produits qui vont s’en trouver renforcées. Ainsi que la maîtrise technique des équipes R&amp;D. Exemplaire: Ce qui est d’emblée visible ce sont les métriques d’un code de qualité, tel que peut nous les montrer Sonarqube. Et pour moi c’est une grande fierté d’avoir contribué à ce projet.  Notes: Discret au sens mathématique du terme : qui est clairement délimité, séparé, isolé.       &#8617;    "
    }, {
    "id": 8,
    "url": "http://0.0.0.0:4000/outside-in-tdd-diamond/",
    "title": "L'Outside-in Diamond TDD, ou l'art de mieux tester",
    "body": "2022/01/15 -  Image d’illustration : Copyright 42skillz / Thomas PIERRAIN Le besoin: Développer de bons tests unitaires n’est pas une tâche aisée. Même les développeurs les plus expérimentés peuvent tomber dans les pièges “classiques” de la pratique du développement des tests automatisés. Les plus courants sont :  Les tests fragiles : ce que l’on teste dans ceux-ci est souvent l’implémentation, dû à l’utilisation excessive de mocks. Lors d’un refactoring, ils devront être mis à jour systématiquement et ce sera coûteux.  Les tests “inutiles” : ce sont des tests qui couvrent du code trivial, et qui n’apportent pas de plus-value au projet. Ce sont des tests qui ne sont pas orientés métier. Par exemple : un test qui va tester une classe de mapping.  Les tests trop complexes : ceux-ci sont souvent des tests d’intégration qui nécessitent beaucoup de plomberie (liée aux prérequis) que l’on ne voit pas. De ce fait, ils deviennent rapidement très difficiles à maintenir. Ils sont souvent très lents à exécuter, difficiles à comprendre et portent sur du code complexe avec de nombreuses lignes de code. Mais alors, comment peut-on réaliser des tests plus robustes ? Pour cela, laissez-moi vous présenter l’Outside-in Diamond TDD. Qu’est ce que l’Outside-in Diamond TDD ?: L’Outside-in Diamond TDD est une technique d’approche des tests unitaires mise au point par Thomas Pierrain. L’idée de base est née d’une constatation : la notion de tests unitaires est mal comprise par la majorité des développeurs. En effet, ceux-ci pensent qu’un test unitaire est un bout de code qui va tester un petit composant en isolation des autres. Or, comme le fait si bien remarquer Thomas Pierrain, la définition de Kent Beck est beaucoup plus juste :  Tests that “runs in isolation” from other tests (des tests qui s’exécutent en isolation des autres tests) Un bon test unitaire s’isole donc lui-même des autres tests, il n’isole pas le composant qu’il teste des autres composants. Cela veut dire que l’on peut faire des tests unitaires qui ne testent pas qu’une classe. Ce n’est pas tout : un bon test unitaire s’isole avant tout des dépendances externes (base de données, disque, API externe…). De plus, Thomas Pierrain a constaté que la pyramide des tests est souvent utilisée de manière dogmatique sans que les développeurs ne se posent la question de ce qui est pertinent à tester. Pendant longtemps, il a cherché à lutter pour faire oublier ces fausses idées, sans succès. J’aime bien sa référence à la loi d’Alberto Brandolini à propos de ce sujet :  “The amount of energy needed to refute bullshit is an order of magnitude larger than to produce it” Donc plutôt que de lutter inutilement pour faire admettre ces idées, Thomas Pierrain est parti sur l’idée de promouvoir les tests d’acceptation. En effet, ceux-ci sont plus faciles à faire accepter pour voir le système à tester comme une boîte noire. Les développeurs auront alors plutôt tendance à tester en termes de contrat métier, ce qui est plus sain pour la maintenabilité des projets. Outside-in: La notion d’Outside-in TDD est assez simple à comprendre : on teste de l’extérieur en allant vers l’intérieur. On commence donc par écrire des tests d’acceptation “gros grains” qui sont orientés métier et testent le système en boîte noire. Pendant la phase red de TDD portant sur ces tests gros grains, on peut être amené à réaliser des boucles TDD plus petites sur l’intérieur du système, typiquement sur le modèle du domaine. A la fin, une fois toutes ces petites boucles réalisées, la boucle principale “gros grain” deviendra alors passante. L’énorme avantage de cette approche par rapport à l’inside out (beaucoup plus répandu) est que l’on ne crée pas de test ou de code de production qui ne correspond pas à un besoin métier. Cela n’est pas sans rappeler le principe YAGNI (You Ain’t Gonna Need It). Pas de superflu ! Diamond: La notion de Diamond vient du fait que les tests réalisés changent complètement la pyramide de test telle que nous la connaissons. Le diamant est fait pour symboliser l’importance et la prépondérance de ces tests d’acceptation par rapport aux autres types de tests (cf. image en haut de l’article) : tests unitaires, tests d’intégration, tests end-to-end. Ce style de TDD nous oriente donc à écrire plus de tests haut niveau et moins de tests dans le détail. Pourquoi l’Outside-in Diamond TDD est si intéressant ?: Cela fait quelques années maintenant que j’utilise l’architecture hexagonale dans mes projets en production. J’ai toujours privilégié les tests unitaires au niveau de mon modèle de domaine, en les construisant à partir des différents cas manipulés par mes services applicatifs. Le schéma ci-dessous représente en vert la partie couverte par les tests unitaires tels que je les réalisais : Cela pose un problème majeur. Il y a tout une partie de code qui se retrouve non testée : les adaptateurs de gauche (API REST par exemple) et les adaptateurs de droite (base de données par exemple). Ces parties sont matérialisées en jaune sur le schéma. Bien sûr, ils étaient couverts par les tests d’intégration, mais les tests d’intégration sont lents. Thomas Pierrain nous fait le retour d’expérience suivant : la plupart des bugs subtils proviennent de ce code non testé, présent dans les adaptateurs. Il préconise donc de tester tout l'hexagone, en partant des adaptateurs de gauche, ainsi qu’en incluant les adaptateurs de droite (en mockant ou fakant uniquement les I/O, que ce soit de la base de données, du fichier ou du réseau). C’est là tout l’intérêt de ce pattern de tests : on a des tests à la fois rapides et qui couvrent largement notre base de code. On a alors un code couvert comme cela : Voyons maintenant un exemple de code pour ces tests (gestion de panier sur un site de e-commerce) : 400: Invalid requestConclusion: Finalement, qu’est ce que c’est qu’un “bon” test unitaire ? Il existe autant de définitions d’un bon test qu’il existe de développeurs. Pour moi, un bon test unitaire est un test qui va tester un contrat métier, sans se préoccuper de l’implémentation. De cette manière, il pourra survivre à tous les refactorings. Mais ce n’est pas tout. Un bon test unitaire doit être parlant. Il doit être concis. Malheureusement, le code des tests unitaires est souvent considéré comme moins important que le code de production et c’est une erreur. Il faut qu’il soit traité avec la plus grande attention. Je vais même aller plus loin : si un test est bien écrit, il sera la documentation de votre use case que vous n’écrirez jamais. Personnellement, j’aime bien l’approche proposée par Vladimir Khorikov pour définir la valeur d’un test. Pour lui, les 4 piliers fondamentaux d’un bon test sont :  qu’il doit permettre d’intercepter une régression qu’il doit résister aux refactorings (grâce au fait qu’il ne soit pas lié à l’implémentation du système testé) qu’il doit fournir un feedback très rapide qu’il doit avoir un coût de maintenance faibleJe trouve que les tests réalisés en Outside-in Diamond TDD remplissent très bien ces fonctions :  ils interceptent bien les régressions car ce sont des tests d’acceptation orientés métier qui testent du comportement. Si celui-ci change, alors on le sait immédiatement.  ils résistent au refactoring car on teste en boîte noire l’hexagone complet en partant des adaptateurs de gauche ils sont rapides, car on utilise le moins de frameworks possible et on mock les I/O (BDD, système de fichiers, réseau…) ils ont un coût de maintenance faible de par leur nature “métier”Un autre avantage indéniable que je trouve à ce pattern de TDD : il permet d’écrire moins de tests et mieux. Chez Primobox, nous mettons en place ce pattern de tests afin de pérenniser nos projets sur le long terme. Sources :  Outside-in Diamond 🔷 TDD #1 - a style made from (&amp; for) ordinary people Outside-in Diamond 🔷 TDD #2 (anatomy of a style) Tech Lead Journal #58 - Principles for Writing Valuable Unit Tests - Vladimir Khorikov"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>R&eacute;sultats de recherche pour '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Pas de r&eacute;sultats...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Fermer"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Fermer</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>R&eacute;sultats de recherche pour '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>D&eacute;sol&eacute;, pas de r&eacute;sultats trouv&eacute;s. Fermez et essayez une recherche diff&eacute;rente !</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});